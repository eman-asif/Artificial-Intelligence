{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an explanation of each function in the code:\n",
    "\n",
    "### 1. **`initialize_parameters(input_size, hidden_size, output_size)`**\n",
    "- **Purpose**: This function initializes the weights and biases for the neural network.\n",
    "- **Inputs**:\n",
    "  - `input_size`: Number of input features (2 for this dataset: `X1` and `X2`).\n",
    "  - `hidden_size`: Number of neurons in the hidden layer.\n",
    "  - `output_size`: Number of output neurons (1 for binary classification).\n",
    "- **Outputs**:\n",
    "  - A dictionary containing randomly initialized weights (`W1` and `W2`) and biases (`b1` and `b2`).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **`forward_propagation(X, weights)`**\n",
    "- **Purpose**: Computes the output of the network by propagating inputs through the layers.\n",
    "- **Steps**:\n",
    "  1. Compute the weighted sum of inputs for the hidden layer (`Z1`).\n",
    "  2. Apply the activation function (tanh) to get the hidden layer output (`A1`).\n",
    "  3. Compute the weighted sum of hidden layer outputs for the output layer (`Z2`).\n",
    "  4. Apply the sigmoid activation function to get the final output (`A2`).\n",
    "- **Inputs**:\n",
    "  - `X`: Input data matrix.\n",
    "  - `weights`: The current weights and biases of the network.\n",
    "- **Outputs**:\n",
    "  - `A2`: Final predictions (output layer).\n",
    "  - `cache`: A dictionary storing intermediate values (`Z1`, `A1`, `Z2`, `A2`), which are reused in backward propagation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **`compute_loss(y_true, y_pred)`**\n",
    "- **Purpose**: Calculates the binary cross-entropy loss, which measures the error in predictions.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  \\text{Loss} = -\\frac{1}{m} \\sum \\left( y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right)\n",
    "  \\]\n",
    "  where \\(m\\) is the number of samples.\n",
    "- **Inputs**:\n",
    "  - `y_true`: Actual labels of the dataset.\n",
    "  - `y_pred`: Predicted probabilities from the network.\n",
    "- **Outputs**:\n",
    "  - A scalar value representing the average loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **`backward_propagation(X, y, weights, cache)`**\n",
    "- **Purpose**: Computes gradients for the weights and biases by applying the chain rule of derivatives.\n",
    "- **Steps**:\n",
    "  1. Compute the gradient of the loss with respect to the output layer (`dZ2`).\n",
    "  2. Calculate gradients for the output layer weights (`dW2`) and biases (`db2`).\n",
    "  3. Propagate the gradient back to the hidden layer (`dZ1`).\n",
    "  4. Calculate gradients for the hidden layer weights (`dW1`) and biases (`db1`).\n",
    "- **Inputs**:\n",
    "  - `X`: Input data matrix.\n",
    "  - `y`: Actual labels.\n",
    "  - `weights`: Current weights and biases.\n",
    "  - `cache`: Intermediate values from forward propagation.\n",
    "- **Outputs**:\n",
    "  - A dictionary containing gradients for all weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **`update_parameters(weights, gradients, learning_rate)`**\n",
    "- **Purpose**: Updates the weights and biases using gradient descent.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  W = W - \\text{learning_rate} \\cdot \\text{gradient}\n",
    "  \\]\n",
    "- **Inputs**:\n",
    "  - `weights`: Current weights and biases.\n",
    "  - `gradients`: Gradients computed from backward propagation.\n",
    "  - `learning_rate`: Step size for updating weights.\n",
    "- **Outputs**:\n",
    "  - Updated weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **`train_network(X, y, hidden_size, learning_rate, epochs)`**\n",
    "- **Purpose**: Orchestrates the training process by iterating over multiple epochs and updating weights.\n",
    "- **Steps**:\n",
    "  1. Initialize weights and biases.\n",
    "  2. Perform forward propagation to compute predictions.\n",
    "  3. Calculate the loss.\n",
    "  4. Perform backward propagation to compute gradients.\n",
    "  5. Update weights using gradient descent.\n",
    "  6. Optionally print loss after every 100 epochs.\n",
    "- **Inputs**:\n",
    "  - `X`: Input data matrix.\n",
    "  - `y`: Actual labels.\n",
    "  - `hidden_size`: Number of neurons in the hidden layer.\n",
    "  - `learning_rate`: Step size for gradient descent.\n",
    "  - `epochs`: Number of training iterations.\n",
    "- **Outputs**:\n",
    "  - Trained weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **`plot_decision_boundary(X, y, weights)`**\n",
    "- **Purpose**: Visualizes how the trained network classifies the dataset by plotting the decision boundary.\n",
    "- **Steps**:\n",
    "  1. Create a grid of points covering the feature space.\n",
    "  2. Use the trained network to predict the class for each grid point.\n",
    "  3. Plot the grid with the predicted classes as a decision boundary.\n",
    "  4. Overlay the original data points with their true labels.\n",
    "- **Inputs**:\n",
    "  - `X`: Input data matrix.\n",
    "  - `y`: Actual labels.\n",
    "  - `weights`: Trained weights and biases.\n",
    "- **Outputs**:\n",
    "  - A 2D plot showing the decision boundary and data points.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you need further explanation about any function or concept!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the answers to the conceptual questions:\n",
    "\n",
    "---\n",
    "\n",
    "### **Forward Propagation**\n",
    "1. **What is the purpose of forward propagation in a neural network?**  \n",
    "   Forward propagation computes the output of the neural network by applying weights, biases, and activation functions to the input data. It is used to make predictions and calculate the loss.\n",
    "\n",
    "2. **How do activation functions like sigmoid and tanh affect the output of a layer?**  \n",
    "   Activation functions introduce non-linearity to the network, allowing it to model complex relationships. Sigmoid maps the output to the range (0, 1), while tanh maps it to (-1, 1), making them useful in different scenarios.\n",
    "\n",
    "3. **Why do we add biases to the weighted sums in forward propagation?**  \n",
    "   Biases help shift the activation function, allowing the network to better fit the data and avoid being constrained to pass through the origin.\n",
    "\n",
    "4. **What is the difference between linear and non-linear activation functions, and why do we prefer the latter in hidden layers?**  \n",
    "   Linear activation functions do not allow the network to learn complex patterns since a combination of linear layers is still linear. Non-linear activation functions, like ReLU or tanh, allow the network to model non-linear relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### **Loss Function**\n",
    "5. **Why is binary cross-entropy used as the loss function for binary classification tasks?**  \n",
    "   Binary cross-entropy measures the distance between the predicted probability and the actual label. It penalizes incorrect predictions more heavily, making it suitable for binary classification.\n",
    "\n",
    "6. **What does a high or low value of binary cross-entropy loss indicate about the performance of the model?**  \n",
    "   A high loss indicates poor predictions, while a low loss indicates that the model's predictions are close to the actual labels.\n",
    "\n",
    "7. **How is the predicted probability (\\(y_{\\text{pred}}\\)) interpreted in binary classification?**  \n",
    "   The predicted probability represents the likelihood of the positive class. For example, \\(y_{\\text{pred}} > 0.5\\) can be interpreted as predicting the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "### **Backward Propagation**\n",
    "8. **What is the main goal of backward propagation?**  \n",
    "   The goal is to calculate gradients of the loss function with respect to the weights and biases to update them and minimize the loss.\n",
    "\n",
    "9. **How does the chain rule help compute gradients during backward propagation?**  \n",
    "   The chain rule allows gradients to be computed layer by layer by propagating the error backward through the network.\n",
    "\n",
    "10. **What happens if gradients are not properly calculated in a neural network?**  \n",
    "   If gradients are incorrect, the weights will not be updated correctly, leading to poor performance or convergence to the wrong solution.\n",
    "\n",
    "11. **Why does backward propagation use intermediate values (like \\(A1\\), \\(Z1\\)) from forward propagation?**  \n",
    "   These values are required to compute gradients efficiently using the chain rule. They represent intermediate computations in the forward pass.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient Descent**\n",
    "12. **What role does the learning rate play in the training process?**  \n",
    "   The learning rate determines the step size for updating weights. It controls how quickly or slowly the model converges to a solution.\n",
    "\n",
    "13. **What could happen if the learning rate is too high or too low?**  \n",
    "   A high learning rate can cause the model to overshoot the minimum, while a low learning rate can lead to very slow convergence.\n",
    "\n",
    "14. **How does gradient descent ensure that the weights converge to a minimum of the loss function?**  \n",
    "   By iteratively updating weights in the opposite direction of the gradient, gradient descent reduces the loss and finds the minimum.\n",
    "\n",
    "15. **Explain the difference between stochastic, batch, and mini-batch gradient descent.**  \n",
    "   - Stochastic Gradient Descent (SGD): Updates weights after each training example.  \n",
    "   - Batch Gradient Descent: Updates weights after processing the entire dataset.  \n",
    "   - Mini-batch Gradient Descent: Updates weights after processing a subset of the dataset, combining the benefits of both SGD and batch gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Training**\n",
    "16. **Why do we need to train a neural network for multiple epochs?**  \n",
    "   Training over multiple epochs allows the model to repeatedly see the data, refining its weights and improving performance.\n",
    "\n",
    "17. **How does the choice of the number of hidden neurons affect the performance of the model?**  \n",
    "   Too few neurons can lead to underfitting, while too many neurons can lead to overfitting.\n",
    "\n",
    "18. **What could cause a model to underfit or overfit the training data?**  \n",
    "   - Underfitting: Model is too simple (e.g., not enough layers/neurons or insufficient training).  \n",
    "   - Overfitting: Model is too complex and learns noise in the training data instead of general patterns.\n",
    "\n",
    "19. **How does initializing weights randomly help avoid symmetry problems during training?**  \n",
    "   Random initialization ensures that different neurons compute different gradients, preventing them from learning the same features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization and Decision Boundary**\n",
    "20. **What is a decision boundary, and why is it important in classification tasks?**  \n",
    "   A decision boundary is the line or surface that separates classes in the feature space. It is important because it shows how the model divides the input space.\n",
    "\n",
    "21. **How can visualizing the decision boundary help evaluate the performance of a neural network?**  \n",
    "   It helps identify how well the model separates classes and whether it generalizes to unseen data.\n",
    "\n",
    "22. **Why might the decision boundary be non-linear for certain datasets?**  \n",
    "   Non-linear boundaries are required when the data itself is non-linearly separable (e.g., XOR problem).\n",
    "\n",
    "---\n",
    "\n",
    "### **General Neural Network Concepts**\n",
    "23. **Why do neural networks with hidden layers perform better than simple linear models for non-linear problems?**  \n",
    "   Hidden layers with non-linear activation functions allow the network to model complex, non-linear relationships in the data.\n",
    "\n",
    "24. **What are the differences between a single-layer perceptron and a multi-layer perceptron?**  \n",
    "   - Single-layer perceptron: Only one layer, can only solve linearly separable problems.  \n",
    "   - Multi-layer perceptron: Has hidden layers and can solve non-linear problems.\n",
    "\n",
    "25. **Why do we prefer to use activation functions like sigmoid or tanh in the output layer for binary classification tasks?**  \n",
    "   These functions produce outputs in a fixed range (e.g., 0-1 for sigmoid), which can be interpreted as probabilities for binary classification.\n",
    "\n",
    "26. **How does increasing the number of hidden layers affect the capacity of a neural network to learn?**  \n",
    "   More hidden layers increase the network's capacity to learn complex patterns but may also increase the risk of overfitting if not regularized properly.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like clarification or further details on any of these answers!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
