{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an explanation of each function in the code:\n",
    "\n",
    "### 1. **`initialize_parameters(input_size, hidden_size, output_size)`**\n",
    "- **Purpose**: This function initializes the weights and biases for the neural network.\n",
    "- **Inputs**:\n",
    "  - `input_size`: Number of input features (2 for this dataset: `X1` and `X2`).\n",
    "  - `hidden_size`: Number of neurons in the hidden layer.\n",
    "  - `output_size`: Number of output neurons (1 for binary classification).\n",
    "- **Outputs**:\n",
    "  - A dictionary containing randomly initialized weights (`W1` and `W2`) and biases (`b1` and `b2`).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **`forward_propagation(X, weights)`**\n",
    "- **Purpose**: Computes the output of the network by propagating inputs through the layers.\n",
    "- **Steps**:\n",
    "  1. Compute the weighted sum of inputs for the hidden layer (`Z1`).\n",
    "  2. Apply the activation function (tanh) to get the hidden layer output (`A1`).\n",
    "  3. Compute the weighted sum of hidden layer outputs for the output layer (`Z2`).\n",
    "  4. Apply the sigmoid activation function to get the final output (`A2`).\n",
    "- **Inputs**:\n",
    "  - `X`: Input data matrix.\n",
    "  - `weights`: The current weights and biases of the network.\n",
    "- **Outputs**:\n",
    "  - `A2`: Final predictions (output layer).\n",
    "  - `cache`: A dictionary storing intermediate values (`Z1`, `A1`, `Z2`, `A2`), which are reused in backward propagation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **`compute_loss(y_true, y_pred)`**\n",
    "- **Purpose**: Calculates the binary cross-entropy loss, which measures the error in predictions.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  \\text{Loss} = -\\frac{1}{m} \\sum \\left( y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right)\n",
    "  \\]\n",
    "  where \\(m\\) is the number of samples.\n",
    "- **Inputs**:\n",
    "  - `y_true`: Actual labels of the dataset.\n",
    "  - `y_pred`: Predicted probabilities from the network.\n",
    "- **Outputs**:\n",
    "  - A scalar value representing the average loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **`backward_propagation(X, y, weights, cache)`**\n",
    "- **Purpose**: Computes gradients for the weights and biases by applying the chain rule of derivatives.\n",
    "- **Steps**:\n",
    "  1. Compute the gradient of the loss with respect to the output layer (`dZ2`).\n",
    "  2. Calculate gradients for the output layer weights (`dW2`) and biases (`db2`).\n",
    "  3. Propagate the gradient back to the hidden layer (`dZ1`).\n",
    "  4. Calculate gradients for the hidden layer weights (`dW1`) and biases (`db1`).\n",
    "- **Inputs**:\n",
    "  - `X`: Input data matrix.\n",
    "  - `y`: Actual labels.\n",
    "  - `weights`: Current weights and biases.\n",
    "  - `cache`: Intermediate values from forward propagation.\n",
    "- **Outputs**:\n",
    "  - A dictionary containing gradients for all weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **`update_parameters(weights, gradients, learning_rate)`**\n",
    "- **Purpose**: Updates the weights and biases using gradient descent.\n",
    "- **Formula**:\n",
    "  \\[\n",
    "  W = W - \\text{learning_rate} \\cdot \\text{gradient}\n",
    "  \\]\n",
    "- **Inputs**:\n",
    "  - `weights`: Current weights and biases.\n",
    "  - `gradients`: Gradients computed from backward propagation.\n",
    "  - `learning_rate`: Step size for updating weights.\n",
    "- **Outputs**:\n",
    "  - Updated weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **`train_network(X, y, hidden_size, learning_rate, epochs)`**\n",
    "- **Purpose**: Orchestrates the training process by iterating over multiple epochs and updating weights.\n",
    "- **Steps**:\n",
    "  1. Initialize weights and biases.\n",
    "  2. Perform forward propagation to compute predictions.\n",
    "  3. Calculate the loss.\n",
    "  4. Perform backward propagation to compute gradients.\n",
    "  5. Update weights using gradient descent.\n",
    "  6. Optionally print loss after every 100 epochs.\n",
    "- **Inputs**:\n",
    "  - `X`: Input data matrix.\n",
    "  - `y`: Actual labels.\n",
    "  - `hidden_size`: Number of neurons in the hidden layer.\n",
    "  - `learning_rate`: Step size for gradient descent.\n",
    "  - `epochs`: Number of training iterations.\n",
    "- **Outputs**:\n",
    "  - Trained weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **`plot_decision_boundary(X, y, weights)`**\n",
    "- **Purpose**: Visualizes how the trained network classifies the dataset by plotting the decision boundary.\n",
    "- **Steps**:\n",
    "  1. Create a grid of points covering the feature space.\n",
    "  2. Use the trained network to predict the class for each grid point.\n",
    "  3. Plot the grid with the predicted classes as a decision boundary.\n",
    "  4. Overlay the original data points with their true labels.\n",
    "- **Inputs**:\n",
    "  - `X`: Input data matrix.\n",
    "  - `y`: Actual labels.\n",
    "  - `weights`: Trained weights and biases.\n",
    "- **Outputs**:\n",
    "  - A 2D plot showing the decision boundary and data points.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you need further explanation about any function or concept!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
